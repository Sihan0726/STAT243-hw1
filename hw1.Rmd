---
title: "STAT-HW1"
author: "Sihan Chen"
date: "2018/09/01"
output: pdf_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

3.First we need to download the files from 2016 to 2018
```{bash}
for i in {2016..2018};do
   curl -o ${i}.csv.gz https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/${i}.csv.gz
done
```
Then we need to unzip the files
```{bash}
gunzip *.csv.gz
```

(a).Now we need to use wc -l to count the observations in each year
```{bash}
for i in {2016..2018};do
   echo "${i} has$(wc -l < ${i}.csv) observations."
done
```

(b).First we still need to download ghcnd-stations.txt
```{bash}
curl -o ghcnd-stations.txt https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt
```
Then we need to get the code from the ghcnd-stations.txt, after that we can grep the lines we need from the three files and put them into a new file named TMAX.txt.
```{bash}
ID=$(grep 'DEATH VALLEY' ghcnd-stations.txt | cut -d' ' -f1)
echo $ID
for i in {2016..2018};do
   grep $ID ${i}.csv | grep TMAX | grep ${i}03 >> TMAX.txt
done
```
After that we can grep the lines we need from the three files and put them into a new file named TMAX.txt

(c).I read the TMAX.txt file into data, and put the max temperature and day value into data_m, and put the same day in March from different years together and draw the side by side boxplot
```{r}
library('ggplot2')
data=read.table("TMAX.txt",sep=",")
data_m=data[c(2,4)]
data_m[1]=data_m[1]%%1000
data_m=data_m[which(data_m[1]>=300) && which(data_m[1]<400),]
data_m[1]=data_m[1]%%100
data_m=data_m[order(data_m[1]),]
names(data_m)=c("day","max_temperature")
p<-ggplot(data=data_m, aes(x=factor(day),y=max_temperature))+geom_boxplot()
p
```

(d).First we need to define a function
```{bash}
function get_weather(){
    if [ "$1" == "-h" ];then
        echo "get_weather usage:get_weather location weather year month filename"
    else
        if [ "$#" != "5" ];then
            echo "Wrong arguments number"
        else
            for i in ${3};do
                curl -o ${i}.csv.gz https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/${i}.csv.gz
                if [ ! -e ${i}.csv ];then
                    rm -f ${i}.csv
                fi
                gunzip ${i}.csv.gz
            done
            curl -o ghcnd-stations.txt https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt
            code=$(grep -h "${1}" ghcnd-stations.txt | cut -d' ' -f1)
            if [ -n $code ];then
                if [ -e $5 ];then
                    rm -f $5
                fi
                for i in ${3};do
                    grep $code ${i}.csv | grep -h ${2} | grep -h "${3}${4}" >> $5
                done
                if [ -s $5 ];then
                    echo "finished"
                else
                    echo "None fits"
                fi
            else
                echo 'Wrong location'
            fi
            for i in ${3};do
                rm -f ${i}.csv
                rm -f ${i}.csv.gz
            done
            rm -f ghcnd-stations.txt
        fi
    fi
}
get_weather 'DEATH VALLEY' TMAX 2016 03 TMAX1.txt
```

4. First we need to extract the original html code from the website
```{bash}
curl https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ -o daily
```
Then we need to extract all the txt files from the html. We find that each txt files' names is in a pair of quotes, so we extract all the data in the quotes and then extract those contains .txt. Finally we use curl to download these files from the website.
```{bash}
files=$(egrep -o '".*?"' daily | grep ".txt" | tr -d '"')
for i in $files;do
    echo "Now we are downloading ${i}."
    curl -o ${i} https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/${i}
done
rm -f daily
```












